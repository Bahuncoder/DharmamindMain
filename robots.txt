# Robots.txt for DharmaMind AI - Professional SEO Configuration

# Allow all legitimate search engines
User-agent: *
Allow: /

# Block AI scrapers and content harvesters to protect intellectual property
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Claude-Web
Disallow: /

User-agent: Amazonbot
Disallow: /

User-agent: Applebot-Extended
Disallow: /

User-agent: Bytespider
Disallow: /

User-agent: meta-externalagent
Disallow: /

User-agent: FacebookBot
Disallow: /

User-agent: facebookexternalhit
Allow: /

# Block malicious crawlers
User-agent: SemrushBot
Disallow: /

User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

# Allow social media crawlers for sharing
User-agent: Twitterbot
Allow: /

User-agent: LinkedInBot
Allow: /

User-agent: WhatsApp
Allow: /

User-agent: TelegramBot
Allow: /

# Sitemap location
Sitemap: https://dharmamind.ai/sitemap_new.xml

# Crawl delay to be respectful of server resources
Crawl-delay: 1

# Specific directives for Google
User-agent: Googlebot
Allow: /
Crawl-delay: 0

# Specific directives for Bing
User-agent: Bingbot
Allow: /
Crawl-delay: 1

# Disallow access to sensitive files and directories
Disallow: /admin/
Disallow: /config/
Disallow: /logs/
Disallow: /*.log$
Disallow: /*.env$
Disallow: /backup/
Disallow: /test/
Disallow: /staging/
Disallow: /.git/
Disallow: /node_modules/
Disallow: /__pycache__/

# Additional crawl directives
# Clean param: remove tracking parameters from URLs in search results
# This helps prevent duplicate content issues
Clean-param: utm_source&utm_medium&utm_campaign&utm_term&utm_content

# Host directive (if you have a preferred domain)
Host: https://dharmamind.ai